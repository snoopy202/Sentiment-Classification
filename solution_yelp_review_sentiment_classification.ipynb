{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiM6gYg0nhkY"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>MAKE A COPY OF THIS NOTEBOOK SO YOUR EDITS ARE SAVED</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svLhCiU3Evm6"
      },
      "source": [
        "# Introduction to AI and Sentiment Analysis with Yelp Reviews\n",
        "\n",
        "Today, we will develop a machine learning model to determine sentiments expressed in Yelp reviews, classifying them as either positive or negative. This introduces the concept of **sentiment analysis**, a form of natural language processing (NLP) that quantifies individuals' opinions (i.e. **good or bad**) from their textual expressions.\n",
        "\n",
        "<!-- ---\n",
        "\n",
        "**Discussion Prompt:** Consider other contexts in which sentiment analysis could be beneficial for businesses or organizations. How might they leverage this technology?\n",
        "\n",
        "--- -->\n",
        "\n",
        "In this notebook, we'll:\n",
        "\n",
        "1. Explore and manipulate a real Yelp review dataset.\n",
        "2. Preprocess text data with tokenization and vectorization.\n",
        "3. Learn word embeddings using pre-trained models.\n",
        "4. Build and train an RNN for sentiment analysis.\n",
        "5. Evaluate the model's performance on unseen data.\n",
        "\n",
        "\n",
        "<!-- * **Explore and manipulate data:** Get hands-on experience with the Yelp review dataset created directly from real reviews from Yelp.\n",
        "* **Preprocess text data:** Learn to convert text into a format suitable for NLP tasks through tokenization and vectorization.\n",
        "* **Introduction to word embeddings:** Utilize pre-trained models to transform words into numerical representations.\n",
        "* **Build and train a model:** Implement a recurrent neural network (RNN) to analyze text data and predict sentiments.\n",
        "* **Evaluate and iterate:** Test the model's performance on unseen data! -->\n",
        "\n",
        "**Discussion Prompt:** Consider other contexts in which sentiment analysis could be beneficial for businesses or organizations. How might they leverage this technology?\n",
        "\n",
        "By the end of this, you will not only be able to build a sentiment analysis classifier but also gain insights into the practical challenges and decisions that come with developing AI models.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZgELVS8I6kc"
      },
      "source": [
        "<center> <img src=https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%203%20-%20NLP/Taco%20Bell%20Reviews.png> </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2jS5ThMCEvnC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import our libraries and data (Make sure you use a GPU runtime!)\n",
        "import pandas as pd   # Great for tables (google spreadsheets, microsoft excel, csv).\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import wordcloud\n",
        "import os # Good for navigating your computer's files\n",
        "import sys\n",
        "pd.options.mode.chained_assignment = None #suppress warnings\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!python -m spacy download en_core_web_md\n",
        "import en_core_web_md\n",
        "text_to_nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "import scipy\n",
        "# from scipy.spatial.distance import cosine\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cosine(word1, word2):\n",
        "\n",
        "  vector1 = word1.reshape(1, -1)\n",
        "  vector2 = word2.reshape(1, -1)\n",
        "\n",
        "  return cosine_similarity(vector1, vector2)[0][0]\n",
        "\n",
        "\n",
        "# Import our data\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%203%20-%20NLP/yelp_final.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ267zCBOjet"
      },
      "source": [
        "# üîç Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BLs_2JkEvnw"
      },
      "source": [
        "First, let's start by loading our review data. The data is stored in a file named `yelp_final.csv`. You can see this file for yourself by clicking the folder icon on the left-hand side of the screen. We will use the `read_csv` function from the pandas library to load the data! Good times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dZ_lymcN_K9"
      },
      "outputs": [],
      "source": [
        "# read our data in using 'pd.read_csv('file')'\n",
        "yelp_full = pd.read_csv('yelp_final.csv')\n",
        "yelp_full.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjL5FrSLEvoP"
      },
      "source": [
        "üí¨ **Discussion:**\n",
        "\n",
        "- **Output Variable Identification:** Which column in the dataset represents the user's sentiment about the restaurant? Think about how the data in this column could be used as a label (i.e. good or bad) for training our model.\n",
        "\n",
        "- **Input Variable Identification:** Which column in the dataset represents the user's review about the restaurant?\n",
        "\n",
        "- **Privacy Considerations:**\n",
        "   - Notice that the business and user identifiers are not real names but appear as random strings. This technique is known as [hashing](https://medium.com/tech-tales/what-is-hashing-6edba0ebfa67), a common method to ensure privacy.\n",
        "   - Discuss why you think real names are not included in this dataset. What are the potential risks of using real names in publicly available data?\n",
        "\n",
        "**Next Steps:**\n",
        "We will keep only the columns necessary for our sentiment analysis. Which are they? Put the columns names in the list below!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution  \n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "* **Output Variable Identification:** The 'stars' column\n",
        "* **Input Variable Identification:** The 'text' column\n",
        "* **Privacy Considerations:** Even if not explicitly in the dataset, a real name can be used to trace back to sensitive information (address, birth date, etc.), leading to potential harrassment, identity theft, employment risks, & reputational damage.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pmz5VTcgkT8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB1yKcUtcpg9"
      },
      "outputs": [],
      "source": [
        "needed_columns = []  # Replace the empty list with the column names as strings\n",
        "\n",
        "# Using only the needed columns from the original dataset\n",
        "yelp = yelp_full[needed_columns]\n",
        "yelp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtO1qM2Oqt9P",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "needed_columns = [\"stars\", \"text\"]  # Replace the empty list with the column names as strings\n",
        "\n",
        "# Using only the needed columns from the original dataset\n",
        "yelp = yelp_full[needed_columns]\n",
        "yelp.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExj8roOEvog"
      },
      "source": [
        "\n",
        "Currently, our main focus is on the 'text' column, which contains the reviews. These reviews express the users' sentiments and provide insights into how they felt about the businesses. Let's examine a few of these reviews to gain a better understanding of our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la3rUPKgEvoi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Explore Reviews Based on Star Ratings\n",
        "#@markdown Use this interactive tool to examine how the content of reviews varies with different star ratings.\n",
        "\n",
        "\n",
        "# Set the number of stars to select reviews\n",
        "num_stars =  1 #@param {type:\"integer\"}\n",
        "\n",
        "# Print the first 20 reviews that match the selected star rating\n",
        "print(f\"Displaying the first 20 reviews rated with {num_stars} stars:\\n\")\n",
        "for review_text in yelp[yelp['stars'] == num_stars]['text'].head(20).values:\n",
        "    print(\"\\n\")\n",
        "    print(review_text + \"\\n\")\n",
        "    print(\"\\n\")\n",
        "    print(\"-\"*2000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GZ6NN4yEvos"
      },
      "source": [
        "üí¨ **Discussion:**\n",
        "\n",
        "- **Vocabulary in High Ratings:** What common words or phrases do you find in reviews with high ratings (e.g., 4 or 5 stars)?\n",
        "  \n",
        "- **Vocabulary in Low Ratings:** What words or phrases frequently appear in reviews with low ratings (e.g., 1 or 2 stars)?\n",
        "\n",
        "- **Notable Exceptions:** Can you identify any reviews that contain unexpected phrases or sentiments?\n",
        "\n",
        "**Consider Further:**\n",
        "\n",
        "- **Impact of Language on Perception:** How might the language used in a review influence a reader's perception of the business? Discuss the potential consequences for businesses based on the language used in customer reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructor Solution  \n",
        "<details><summary>click to reveal!</summary>\n",
        "\n",
        "* **Vocabulary in High Ratings:** Words like \"great\", \"love\", \"favorite\" are often used in highly rated reviews\n",
        "* **Vocabulary in Low Ratings:** Words like \"bad\", \"rude\", \"disappointing\" are often used in low rated reviews\n",
        "* **Notable Exceptions:** \"The food is good. Better then Dennys but not as good as Mimi's\" is another example of an exception\n",
        "* **Impact of Language on Perception:** Tone, choice of words, and various factors can negatively or positively impact a business's reputation, customer trust, and subsequent financial success.\n"
      ],
      "metadata": {
        "id": "izCmaYoUMtIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X79o35cF9ZJT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Set the number of stars to select reviews\n",
        "num_stars =  5 #@param {type:\"integer\"}\n",
        "\n",
        "# Get text for star rating\n",
        "text_for_star = yelp[yelp['stars'] == num_stars]['text'].values\n",
        "\n",
        "# Convert text to one string\n",
        "string_text_for_star= str(text_for_star)\n",
        "\n",
        "# Split the string by words\n",
        "words_for_star = string_text_for_star.split()\n",
        "print(f\"Words for {num_stars} star reviews: \", words_for_star)\n",
        "\n",
        "# Pass the list of words to an instance of Counter class\n",
        "Counter = Counter(words_for_star)\n",
        "\n",
        "# Get most common 20 words\n",
        "most_common_words = Counter.most_common(50)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(f\"Most commmon words for {num_stars} star reviews: \", most_common_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpU7nrWTEvov"
      },
      "source": [
        "### üí° Exercise: Crafting Rules for Sentiment Analysis\n",
        "\n",
        "Think about the reviews you've looked at. Imagine you're designing a simple system to tell if a review is **positive** or **negative** based only on what words it uses. This is the basis of a rule-based classifier: it uses specific rules you set to make decisions.\n",
        "\n",
        "For example, reviews containing the word \"good\" could be positive, while those with the word \"bad\" might be negative.\n",
        "\n",
        "As a group, let's come up with set of rules using combinations of words that might help identify the sentiment of a review. Write down your ideas below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxuZyKOKy6Cc",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define Your Sentiment Analysis Rules\n",
        "\n",
        "#@markdown Rule 1: Describe a combination of words or a pattern that typically indicates a positive review.\n",
        "rule_1 = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Rule 2: Describe a combination of words or a pattern that typically indicates a negative review.\n",
        "rule_2 = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Rule 3: Enter an additional rule or an exception you observed.\n",
        "rule_3 = \"\" #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4PQg8FhEvow"
      },
      "source": [
        "**Discuss**:\n",
        "\n",
        "Do you think the rules you've created will perform well in accurately classifying review sentiments? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoAIiKE9WisH"
      },
      "source": [
        "---\n",
        "\n",
        "### üí° Bonus Exercise: Implement and Test Your Sentiment Analysis Rules\n",
        "\n",
        "Now it's time to put your rules to the test! Write a function that uses one of the rules you developed to determine whether a review is **positive** or **negative**. We've provided the basic structure of the function below. Replace the `pass` statement with your own code to implement your rule.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GjzoXUjW0YU"
      },
      "outputs": [],
      "source": [
        "def classify(text):\n",
        "    # YOUR CODE HERE\n",
        "    # Implement your rule to classify the sentiment of the review.\n",
        "    # You might start with a simple 'if' statement checking for certain words or phrases.\n",
        "    pass  # Remove 'pass' and replace it with your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dyy8sD40W-Xp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor  Solution\n",
        "# It checks if the word 'bad' appears in the text.\n",
        "def classify(text):\n",
        "    # If the word 'bad' is found, the function returns 'negative'.\n",
        "    if 'bad' in text:\n",
        "        return 'negative'\n",
        "    # If the word 'bad' is not found, it assumes the review is 'positive'.\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "### More Complex Solution\n",
        "def classify(text):\n",
        "    # This function classifies a review as 'negative', 'neutral', or 'positive'.\n",
        "    # It checks for the presence of specific keywords to determine the sentiment.\n",
        "\n",
        "    # Define lists of positive and negative keywords.\n",
        "    positive_keywords = ['great', 'amazing', 'loved', 'excellent', 'good', 'wonderful']\n",
        "    negative_keywords = ['bad', 'worst', 'disappointing', 'poor', 'terrible', 'awful']\n",
        "\n",
        "    # Count occurrences of positive and negative keywords.\n",
        "    positive_count = sum(word in text for word in positive_keywords)\n",
        "    negative_count = sum(word in text for word in negative_keywords)\n",
        "\n",
        "    # Determine sentiment based on the counts of positive and negative words.\n",
        "    if positive_count > negative_count:\n",
        "        return True\n",
        "    elif negative_count >= positive_count:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf7S02lHsUUq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title  üß™  Let's Test Your Sentiment Classification Function\n",
        "#@markdown Enter your own review text below to see how your function classifies it:\n",
        "\n",
        "# User inputs their own review text.\n",
        "input_review = \"hey there i love this stuff and it is amazing\" #@param {type:\"string\"}\n",
        "\n",
        "# Call the classify function with the user input.\n",
        "if input_review:  # Check if the input string is not empty\n",
        "    sentiment = classify(input_review)\n",
        "    print(f\"Review: {input_review}\")\n",
        "    print(f\"Sentiment: {sentiment}\")\n",
        "else:\n",
        "    print(\"Please enter a review text to classify.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dArbYofKN206"
      },
      "source": [
        "# ‚öôÔ∏è Processing the Data for Machine Learning\n",
        "\n",
        "As we transition from manually crafting rules to employing more sophisticated machine learning techniques, we will prepare our data for analysis using a Recurrent Neural Network (RNN). This type of model is particularly effective for processing sequences, such as text, due to its ability to maintain information across inputs!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bwasn11BfD4"
      },
      "source": [
        "### üí° Exercise: Binary Classification of Review Sentiments\n",
        "\n",
        "Here we want to classify Yelp reviews into two sentiment categories: **positive** and **negative**. To simplify our task into a binary classification problem, we will:\n",
        "\n",
        "- Label reviews with 4 and 5 stars as 'positive'.\n",
        "- Label reviews with 1, 2, and 3 stars as 'negative'.\n",
        "\n",
        "We've already provided the function definition `is_good_review`. Fill in the `None` with the correct expression to corretly divide the dataset into two goups, good or bad!\n",
        "\n",
        "Please complete the function below and run it to create a new `is_good_review` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck4iX6PITzHS"
      },
      "outputs": [],
      "source": [
        "def is_good_review(num_stars):\n",
        "    # This function categorizes reviews based on the number of stars.\n",
        "    # It returns True if the review is positive (4 or 5 stars).\n",
        "    # It returns False if the review is negative (1, 2, or 3 stars).\n",
        "\n",
        "    # Replace 'None' with the appropriate condition for a positive review.\n",
        "    if None:  # YOUR CODE HERE\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kgydiYfuERE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "def is_good_review(num_stars):\n",
        "    # This function categorizes reviews based on the number of stars.\n",
        "    # It returns True if the review is positive (4 or 5 stars).\n",
        "    # It returns False if the review is negative (1, 2, or 3 stars).\n",
        "\n",
        "    # Replace 'None' with the appropriate condition for a positive review.\n",
        "    if num_stars >= 4:  # YOUR CODE HERE\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ_J7L6KtkYQ"
      },
      "outputs": [],
      "source": [
        "# Apply the function to the 'stars' column to create a new 'is_good_review' column.\n",
        "# This column will have a Boolean value where True represents a 'good' review and False represents a 'bad' review.\n",
        "yelp['is_good_review'] = yelp['stars'].apply(is_good_review)\n",
        "\n",
        "# Display the first few rows to verify the changes.\n",
        "yelp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i1Ja6ZfESWr"
      },
      "outputs": [],
      "source": [
        "# FIRST: Make sure your classifier returns True or False (for good vs. bad reviews)\n",
        "\n",
        "# Helper function to show predictions\n",
        "def show_pred(y_test,y_pred):\n",
        "  table=pd.DataFrame([[t for t in reviews],y_pred, y_true]).transpose()\n",
        "  table.columns = ['Text', 'Predicted Category', 'True Category']\n",
        "  accuracy = (sum(table['Predicted Category'] == table['True Category'])/len(table['True Category']))\n",
        "  print(\"Accuracy: {:.2%}\".format(accuracy))\n",
        "  return table\n",
        "\n",
        "reviews = yelp['text']\n",
        "#@title Bonus: test your rule based classifier's accuracy on all the reviews\n",
        "y_true = yelp[\"is_good_review\"]\n",
        "#Use classify_rb to make predictions\n",
        "y_pred = [classify(review) for review in reviews] # a list of predictions\n",
        "#Display the tweet with predicted and True category\n",
        "show_pred(y_true,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pby7LlwhEvpN"
      },
      "source": [
        "## ‚úÇÔ∏è Tokenization\n",
        "\n",
        "The first step in processing text is **tokenization**, which involves breaking down the text from a single string into individual words, or \"tokens.\" Try it out yourself by entering some text into the cell below to see how it's tokenized into a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XOaa1uEEvpY"
      },
      "outputs": [],
      "source": [
        "#@title Basic Tokenization Example\n",
        "#@markdown Enter any text in the field below to see how it is broken down into tokens. Tokenization splits the text into words or symbols.\n",
        "\n",
        "example_text = \"What's the word?\" #@param {type:\"string\"}\n",
        "\n",
        "# Check if the example text is not empty\n",
        "if example_text:\n",
        "    tokens = word_tokenize(example_text)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print(\"Number of tokens:\", len(tokens))\n",
        "else:\n",
        "    print(\"Please enter some text to tokenize.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFR1kisnXXYI"
      },
      "source": [
        "### üí¨ Discussion: Analyzing Tokenization Rules\n",
        "\n",
        "1. **Tokenization Patterns:**\n",
        "   - Observe and discuss the rules the tokenizer follows when splitting text into tokens, especially how it handles punctuation like periods, commas, and hyphens.\n",
        "\n",
        "2. **Evaluation and Modification:**\n",
        "   - How does the tokenizer handle punctuation? What would you change about it?\n",
        "\n",
        "Reflect on these aspects and share your insights!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfIUrRtEWr3H"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## üìö Understanding Word Embeddings with Word2Vec\n",
        "\n",
        "When we work with text in machine learning, we can't use words directly. Instead, we need to convert words into **vectors** or **embeddings**. Think of these as lists of numbers that represent each word in a way that a computer can understand and process.\n",
        "\n",
        "### What is Word2Vec?\n",
        "\n",
        "**Word2Vec** is a popular method to create these embeddings. It was developed by researchers at Google and has become a standard tool in machine learning for handling text. Word2Vec transforms each word into a dense vector that captures much more information about the word than just its meaning. For instance, words that appear in similar contexts, like \"school\" and \"teacher,\" will have vectors that are closer together or are more 'similar'.\n",
        "\n",
        "### How Does Word2Vec Work?\n",
        "\n",
        "Word2Vec models are trained to understand language based on actual sentences. It looks at each word and its neighboring words to predict words from context, or the other way around.\n",
        "\n",
        "\n",
        "### Visualizing Word2Vec Embeddings\n",
        "\n",
        "The image below displays a Word2Vec model's word embeddings plotted in a three-dimensional space. Each dot represents a word, and its position is determined by the similarity its' meaning has to other words. Words that are similar are clustered together. Don't worry about what the axis represents; all the matters is how close they are to each other!\n",
        "\n",
        "[Word2Vec Visualization](https://projector.tensorflow.org/)\n",
        "\n",
        "**Explore the Visualization:**\n",
        "- **Feel free to zoom in on the clusters to observe how closely related the words are.**\n",
        "\n",
        "### Why Use Word2Vec?\n",
        "\n",
        "Word2Vec allows computers to understand words in a more human-like way, recognizing synonyms, related terms, and even grammatical patterns. This ability is incredibly powerful for tasks like translation, search engines, and of course as it applies to us, sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsbhSYwqYMzC"
      },
      "source": [
        "## üîç Exploring Word Embeddings with spaCy\n",
        "\n",
        "Having seen how Word2Vec can visualize semantic relationships between words, let's now apply these concepts practically using the spaCy!\n",
        "\n",
        "### Understanding Our Tools\n",
        "\n",
        "- **Model Overview:** We will use `en_core_web_md`, a medium-sized Word2Vec model provided by spaCy. This model has been trained on a vast corpus of text from the internet, enabling it to understand language by analyzing the contexts in which words appear.\n",
        "- **Helper Function:** We've provided a function `word2vec(word)` that uses this model to convert any given word into its corresponding word embedding. This representation captures the word's semantic essence based on its usage across millions of sentences.\n",
        "\n",
        "You can use the function like this: `vec = word2vec(word)`\n",
        "\n",
        "Let's start by converting words to their emebddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GxKgy5uv1Jz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Click here to load the word2vec function!\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the SpaCy Word2Vec model\n",
        "# spacy.prefer_gpu()\n",
        "text_to_nlp = en_core_web_md.load()\n",
        "\n",
        "# Function to convert a word to its vector representation\n",
        "def word2vec(word):\n",
        "    return text_to_nlp(word).vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY5fq4YadDC5"
      },
      "source": [
        "### üí° Exercise: Word Embeddings\n",
        "\n",
        "Retrieve the word embedding for the word \"student\" using the provided `word2vec` function. Then, discuss the following:\n",
        "\n",
        "- Determine the length of the embedding vector for \"student.\" What does this tell you about the nature of word embeddings?\n",
        "- Compare the length of vectors for different words. Are they consistent across various words?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzpMqkYdYMzD"
      },
      "outputs": [],
      "source": [
        "# Define the word you want to analyze\n",
        "word = \"student\"\n",
        "word2 = \"teacher\"\n",
        "# Retrieve the word embedding vector for the word \"student\"\n",
        "word_embedding = None # Replace None with the function call to get the vector\n",
        "\n",
        "# Get the length of the word embedding vector\n",
        "length_word_embedding = None  # Replace None with code to calculate the length of the vector\n",
        "\n",
        "# Print the word embedding vector and its length\n",
        "print(\"Word Embedding:\", word_embedding)\n",
        "print(\"Length of Word Embedding:\", length_word_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfNRdncnwVNy",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "# Define the word you want to analyze\n",
        "word = \"student\"\n",
        "word2 = \"teacher\"\n",
        "# Retrieve the word embedding vector for the word \"student\"\n",
        "word_embedding = word2vec(word) # Replace None with the function call to get the vector\n",
        "\n",
        "# Get the length of the word embedding vector\n",
        "length_word_embedding = len(word_embedding)  # Replace None with code to calculate the length of the vector\n",
        "\n",
        "# Print the word embedding vector and its length\n",
        "print(\"Word Embedding:\", word_embedding)\n",
        "print(\"Length of Word Embedding:\", length_word_embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2euEDp-YMzE"
      },
      "source": [
        "# ‚úç Similarity Using Word Vectors\n",
        "\n",
        "Word vectors allow us to quantify how similar two words are by comparing their embeddings (i.e. vectors). To measure this similarity, we use the cosine similarity metric, which can be calculated using the function `cosine(vector1, vector2)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCIwyK2WNx2R"
      },
      "source": [
        "![](https://storage.googleapis.com/lds-media/images/cosine-similarity-vectors.original.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GsnUMqQOEn5"
      },
      "source": [
        "### üí° Exercise\n",
        "\n",
        "Your task is to explore the similarity between word pairs:\n",
        "\n",
        "**Compute Similarity:**\n",
        "   - Write code to compute the similarity score between the words \"walk\" and \"run\".\n",
        "\n",
        "**Experiment with Pairs:**\n",
        "   - Find pairs of words where:\n",
        "     - The similarity score is greater than 0.8.\n",
        "     - The similarity score is less than 0.2.\n",
        "     - The similarity score is unexpectedly high or low based on your intuition about the words.\n",
        "\n",
        "Use the cosine similarity function mentioned above to carry out these comparisons, and discuss your findings. Remember, you'll need to transform your words to vectors first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu5iHeVvYMzF"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "def similarity(word1, word2):\n",
        "  pass\n",
        "\n",
        "similarity('walk', 'run')\n",
        "### END CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CEDIXnJw61e",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "def similarity(word1, word2):\n",
        "\n",
        "  vector1 = word2vec(word1)\n",
        "  vector2 = word2vec(word2)\n",
        "\n",
        "  return cosine(vector1, vector2)\n",
        "\n",
        "similarity('student', 'teacher')\n",
        "### END CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0TWV1RLYMzF"
      },
      "source": [
        "## üéì **[Optional]** Advanced Challenge Exercise: Computing Cosine Similarity\n",
        "\n",
        "Cosine similarity measures the cosine of the angle between two non-zero vectors. This is used to assess how close two items are. It ranges from -1 (exactly opposite) to 1 (exactly the same), with 0 typically indicating no similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0aSqswQvzuP"
      },
      "source": [
        "### üìê Cosine Similarity Formula\n",
        "\n",
        "The cosine similarity between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is calculated as follows:\n",
        "\n",
        "$$ \\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} $$\n",
        "\n",
        "Where:\n",
        "- $ \\mathbf{A} \\cdot \\mathbf{B} $ is the dot product of the vectors,\n",
        "- $ \\|\\mathbf{A}\\| $ and $ \\|\\mathbf{B}\\| $ are the norms (or magnitudes) of the vectors. Really, this is just another fancy way of saying \"length\".\n",
        "\n",
        "To successfully implement this, here are some helpful hints regarding the functions and libraries you might need:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vomAid3tv-PJ"
      },
      "source": [
        "#### Functions and Methods to Use:\n",
        "1. **`np.dot()` or `@` operator:** Use this to compute the dot product of two vectors. This function takes two arrays and returns their dot product.\n",
        "   \n",
        "   ```python\n",
        "   dot_product = np.dot(vector1, vector2)\n",
        "   # or\n",
        "   dot_product = vector1 @ vector2\n",
        "   ```\n",
        "\n",
        "2. **`np.linalg.norm()`:** This function computes the norm (magnitude) of a vector. You'll need to calculate the norm for both vectors involved in the cosine similarity.\n",
        "\n",
        "   ```python\n",
        "   norm_vector = np.linalg.norm(vector1)\n",
        "   ```\n",
        "   \n",
        "Use these functions to calculate the cosine similarity according to the formula:\n",
        "\n",
        "$$ \\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BMYUC8E1n3Os"
      },
      "outputs": [],
      "source": [
        "#Your code here! Assume the vectors are numpy arrays already!\n",
        "def my_cosine_similarity(vec1, vec2):\n",
        "    dot_product = None #Fill me in\n",
        "    norm_vec1 = None   #Fill me in\n",
        "    norm_vec2 = None   #Fill me in\n",
        "    similarity = None  #Fill me in\n",
        "    return similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdfAlSxuyra3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "def my_cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUsZqA6zyn54"
      },
      "outputs": [],
      "source": [
        "#@title Run this to check if your function is correct!\n",
        "\n",
        "# Example vectors\n",
        "vector1 = np.array([1, 2, 3])\n",
        "vector2 = np.array([1, 5, 7])\n",
        "\n",
        "# Compute the cosine similarity\n",
        "similarity_score = my_cosine_similarity(vector1, vector2)\n",
        "print(\"Cosine Similarity:\", similarity_score)\n",
        "print(\"Correct answer: 0.9875414397573881\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zb-eRcBSe8Y"
      },
      "source": [
        "## üîÑ Putting it all together: Preparing Your Data for the Model\n",
        "\n",
        "Having explored tokenization and word embeddings, we're now ready to apply these concepts to prepare our data for the model! This step is crucial as it transforms raw text into a structured format that our machine learning algorithms can understand and learn from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMdxEB-kTVHu"
      },
      "source": [
        "### üí° Exercise\n",
        "Identify which columns in the `yelp` dataframe should be used as your X (inputs/features) and which should be your y (outputs/labels). Complete the following code to specify these columns:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6t6HQm1vEvrQ"
      },
      "outputs": [],
      "source": [
        "# Specify the input features (X) and output labels (y) from the 'yelp' dataframe\n",
        "X_columns = \"\" # Replace with the names of column to be used as input features\n",
        "y_column = \"\"  # Replace with the name of the column to be used as output labels\n",
        "\n",
        "\n",
        "X_text = yelp[X_columns]\n",
        "y = yelp[y_column]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyTGJAOTzCvk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "# Specify the input features (X) and output labels (y) from the 'yelp' dataframe\n",
        "X_columns = \"text\" # Replace with the names of columns to be used as input features\n",
        "y_column = \"is_good_review\"  # Replace with the name of the column to be used as output labels\n",
        "\n",
        "\n",
        "X_text = yelp[X_columns]\n",
        "y = yelp[y_column]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osD1DSmXTl_w"
      },
      "source": [
        "##üí° Exercise: Preparing Text Data for Machine Learning\n",
        "\n",
        "In this exercise, you'll use some functions we wrote  to prepare text data for a machine learning model. This involves tokenizing the text, converting it into numerical embeddings, and making sure each text entry is the same length before feeding it into the model!\n",
        "\n",
        "1. **Load and Tokenize Text:**\n",
        "   - Use the function `tokenize_and_embed(text_data)` to process your text data in batches. This function handles the tokenization and conversion of text into embeddings for you.\n",
        "\n",
        "2. **Standardize Text Length:**\n",
        "   - Apply `standardize_length(embeddings)` to ensure that all text data have the same number of features. This function finds the longest text and pads the others accordingly.\n",
        "\n",
        "3. **Convert to Machine Learning Format:**\n",
        "   - Finally, use `convert_to_array(padded_embeddings)` to transform your standardized text data into a format suitable for machine learning models.\n",
        "\n",
        "<!--\n",
        "\n",
        "**Disclaimer: Simplifying Text Processing for Educational Purposes**\n",
        "\n",
        "In this educational module, we are focusing on higher-level concepts and applications of machine learning rather than delving deeply into every preprocessing step. One such step we are simplifying is the process of standardizing the length of text entries before they are fed into a machine learning model.\n",
        "\n",
        "Ensuring that each text entry is the same length is crucial for many machine learning algorithms, especially neural networks, as they require fixed-size inputs. This process, often achieved through padding shorter texts with zeros, can involve intricate choices about text truncation, padding strategies, and the handling of embeddings.\n",
        "\n",
        "However, to keep our focus on the broader application of natural language processing (NLP) techniques and to ensure that students are not overwhelmed by the complexity of data preprocessing, we are using predefined functions to manage this step. This approach allows students to concentrate on understanding how machine learning models operate on text data and the impact of NLP in real-world applications, rather than getting bogged down in the details of text length standardization.\n",
        "\n",
        "By abstracting away these details, we aim to make the learning experience more accessible and engaging, allowing students to build a foundational understanding before tackling more complex aspects of NLP and AI. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zso1fJYg0KC5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run This to Load Our Functions!\n",
        "\n",
        "def tokenize_and_embed(text_data):\n",
        "    \"\"\"\n",
        "    Tokenizes the text data and converts it to word embeddings using SpaCy.\n",
        "    Args:\n",
        "        text_data (list): A list of text strings to be processed.\n",
        "    Returns:\n",
        "        list: A list of lists containing embeddings for each token in each document.\n",
        "    \"\"\"\n",
        "    docs = list(text_to_nlp.pipe(text_data))\n",
        "    embeddings = [[token.vector for token in doc] for doc in docs]\n",
        "    return embeddings\n",
        "\n",
        "def standardize_length(embeddings):\n",
        "    \"\"\"\n",
        "    Ensures all embedding lists are the same length by padding shorter ones with zero vectors.\n",
        "    Args:\n",
        "        embeddings (list): A list of lists of embeddings.\n",
        "    Returns:\n",
        "        list: A list of lists with padded embeddings to ensure uniform length.\n",
        "    \"\"\"\n",
        "    max_length = max(len(tokens) for tokens in embeddings)\n",
        "    embedding_dim = len(embeddings[0][0]) if embeddings[0] else 0\n",
        "    padded_embeddings = [[np.zeros(embedding_dim)] * (max_length - len(tokens)) + tokens for tokens in embeddings]\n",
        "    return padded_embeddings\n",
        "\n",
        "def convert_to_array(padded_embeddings):\n",
        "    \"\"\"\n",
        "    Converts a list of padded embeddings into a numpy array.\n",
        "    Args:\n",
        "        padded_embeddings (list): A list of lists of padded embeddings.\n",
        "    Returns:\n",
        "        numpy.ndarray: A numpy array containing the embeddings suitable for machine learning input.\n",
        "    \"\"\"\n",
        "    return np.array(padded_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar52_VyM0s6Y"
      },
      "outputs": [],
      "source": [
        "# X_text contains our text data to be processed\n",
        "X_embeddings = None  # Tokenize and get embeddings\n",
        "\n",
        "X_padded = None  # Standardize lengths\n",
        "\n",
        "X = None  # Convert to numpy array suitable for model input\n",
        "\n",
        "if X: print(f\"The shape of our dataset is now: {X.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrRiQWZ0u8lb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "# X_text contains our text data to be processed\n",
        "X_embeddings = tokenize_and_embed(X_text)  # Tokenize and get embeddings\n",
        "\n",
        "X_padded = standardize_length(X_embeddings)  # Standardize lengths\n",
        "\n",
        "X = convert_to_array(X_padded)  # Convert to numpy array suitable for model input\n",
        "\n",
        "print(f\"The shape of our dataset is now: {X.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr5XHuLmVciq"
      },
      "source": [
        "## Exercise: Splitting Data into Training and Testing Sets\n",
        "\n",
        "Now that you have processed your reviews, the next step is to divide this data into training and testing sets.\n",
        "\n",
        "Use the `train_test_split()` function to create training and testing datasets.\n",
        "\n",
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=.2, random_state=1)\n",
        "```\n",
        "\n",
        "- **test_size**: This parameter controls the proportion of the data that will be split into the testing set.\n",
        "\n",
        "<!--\n",
        "- **random_state**: Setting this parameter ensures that the split is reproducible. -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PThy6pNUEvsA"
      },
      "outputs": [],
      "source": [
        "#Fill in None!\n",
        "X_train, X_test, y_train, y_test = train_test_split(None, None, test_size=.2, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPLd2Asn16z2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42rl41sUXr0-"
      },
      "source": [
        "\n",
        "# Recurrent Neural Networks (RNNs)\n",
        "\n",
        "We are now going to explore a special type of neural network called a Recurrent Neural Network (RNN). While we have briefly touched on other types of neural networks before, RNNs are unique because they can process sequences of data in order. This makes them particularly useful for tasks where the sequence or the order of data points is important.\n",
        "\n",
        "\n",
        "**How Do RNNs Work?**\n",
        "\n",
        "<img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/rnn-many-to-many-different-ltr.png?8ca8bafd1eeac4e8c961d9293858407b\" width=\"500\">\n",
        "\n",
        "\n",
        "Unlike traditional neural networks, which treat each input independently, RNNs have loops in them that allow information to persist. In simpler terms, RNNs can remember information about what has been processed so far, enabling them to make predictions based on the sequence of data received.\n",
        "\n",
        "**Examples of RNN Applications:**\n",
        "\n",
        "- **Stock Prices Prediction:** RNNs can predict future stock prices by learning from past stock price trends.\n",
        "- **Language Modeling:** They can predict the next word in a sentence based on the words that came before, which is useful in text auto-completion tools.\n",
        "- **Weather Forecasting:** RNNs can predict future weather conditions by analyzing the patterns in past weather data.\n",
        "\n",
        "RNNs are well-suited and indispensable for many tasks in fields like finance, natural language processing, and meteorology!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yANSd0QWWu1"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "We've built the RNN model for you! All you need to do is train it using the .fit() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1zLNp8IRSbp",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run this to load the RNN Model!\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "class RNNClassifier:\n",
        "    def __init__(self, num_epochs=30, lstm_units=50, dropout_rate=0.7):\n",
        "        self.num_epochs = num_epochs\n",
        "        self.lstm_units = lstm_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(self.lstm_units, return_sequences=True))\n",
        "        model.add(Dropout(self.dropout_rate))\n",
        "        model.add(LSTM(self.lstm_units))\n",
        "        model.add(Dropout(self.dropout_rate))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        optimizer = Adam(learning_rate=0.001)\n",
        "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n",
        "\n",
        "        \"\"\"\n",
        "        Comment is necessary due to how over complicated I made this. - joel\n",
        "\n",
        "        Fits the model to the training data. Supports optional validation data.\n",
        "        If validation data is provided, early stopping is used if not it's not! haha\n",
        "\n",
        "        Args:\n",
        "            X_train (array): Training data features.\n",
        "            y_train (array): Training data labels.\n",
        "            X_val (array, optional): Validation data features.\n",
        "            y_val (array, optional): Validation data labels.\n",
        "            **kwargs: Additional keyword arguments to pass to the model's fit method.\n",
        "\n",
        "        Returns:\n",
        "            A history object containing training history.\n",
        "        \"\"\"\n",
        "\n",
        "        if X_train is None and y_train is None:\n",
        "          print(\"Arguments are none. Retry with correct arguments.\")\n",
        "          return None\n",
        "\n",
        "        callbacks = kwargs.pop('callbacks', [])\n",
        "\n",
        "        if X_val is not None and y_val is not None:\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "            callbacks.append(early_stopping)\n",
        "            return self.model.fit(X_train, y_train, epochs=self.num_epochs, validation_data=(X_val, y_val), callbacks=callbacks, batch_size=32, verbose=1, **kwargs)\n",
        "        else:\n",
        "            return self.model.fit(X_train, y_train, epochs=self.num_epochs, batch_size=32, verbose=1, callbacks=callbacks, **kwargs)\n",
        "\n",
        "    def predict(self, *args, **kwargs):\n",
        "        predictions = self.model.predict(*args, **kwargs)\n",
        "        return (predictions > 0.5).astype(int)\n",
        "\n",
        "    def predict_proba(self, *args, **kwargs):\n",
        "        return self.model.predict(*args, **kwargs)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        if name != 'predict' and name != 'predict_proba':\n",
        "            return getattr(self.model, name)\n",
        "        else:\n",
        "            raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK-rAOubzVUi"
      },
      "outputs": [],
      "source": [
        "#YOUR CODE HERE! Replace the nones!\n",
        "rnn = RNNClassifier(num_epochs=30, lstm_units=50, dropout_rate=0.5)\n",
        "rnn.fit(None, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-hp6zgu3MKY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "rnn = RNNClassifier(num_epochs=30, lstm_units=50, dropout_rate=0.5)\n",
        "rnn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hFMR7NUV84D"
      },
      "source": [
        "### ‚úç Exercise: Testing Your Model\n",
        "Now, let's evaluate our model's accuracy! Your model needs to **predict** the sentiment, and then you'll **calculate the accuracy** using the `accuracy_score()` function. **Which dataset** should you use?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = # YOUR CODE HERE\n",
        "accuracy = # YOUR CODE HERE\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "obd1a2L1I1Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVdf84tuWQwM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "y_pred = rnn.predict(X_test) #YOUR CODE HERE\n",
        "accuracy = accuracy_score(y_pred, y_test)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zct-JZPQaXno"
      },
      "source": [
        "Congratulations - you've trained and tested your model! It's not perfect, but a whole lot better than a coin flip :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiUFnTXrased"
      },
      "source": [
        "### ‚úç Exercise: Trying Out Reviews\n",
        "\n",
        "Accuracy only tells us so much! It's often useful to figure out **what sorts** of mistakes your model makes.\n",
        "\n",
        "Try enterning some reviews below and explore:\n",
        "\n",
        "*   What kind of reviews does your model classify correctly? For example, do long or short reviews work better?\n",
        "*   What kind of reviews does your model get wrong? Does it understand sarcasm or other \"tricky\" language?\n",
        "*   Does it seem like your model pays attention to particular words?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euuR1VWWEvsX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Enter a review to see your model's classification\n",
        "example_review = \"This was a horrible place!\" #@param {type:'string'}\n",
        "\n",
        "# Assuming the functions tokenize_and_embed, standardize_length, and convert_to_array are defined in the same script or imported\n",
        "# First, wrap the example review in a list since our functions expect a list of texts\n",
        "example_reviews = [example_review]\n",
        "\n",
        "# Tokenize and convert the review text to embeddings\n",
        "X_embeddings = tokenize_and_embed(example_reviews)  # Tokenize and get embeddings\n",
        "\n",
        "# Standardize lengths of the embeddings\n",
        "X_padded = standardize_length(X_embeddings)  # Standardize lengths\n",
        "\n",
        "# Convert the padded embeddings into a numpy array suitable for the model\n",
        "X = convert_to_array(X_padded)  # Convert to numpy array suitable for model input\n",
        "\n",
        "prediction = rnn.predict(X)\n",
        "if prediction[0]:\n",
        "  print (\"This was a GOOD review!\")\n",
        "else:\n",
        "  print (\"This was a BAD review!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCpibGeVgy2i"
      },
      "source": [
        "#Exploring Impact and Ethics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cAMr90IfcQE"
      },
      "source": [
        "\n",
        "\n",
        "Whenever we explore a new potential use of AI, it is crucial to have a discussion about the **societal and ethical impact** if it were to be implemented at a large scale.\n",
        "\n",
        "*Illustration: erhui1979/iStock*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDboCWKLlh6g"
      },
      "source": [
        "<center> <img src=\"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%201%20-%205/Session%203%20-%20NLP/AI%20Ethics.png\"> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEOJTWJxmEoI"
      },
      "source": [
        "### üìà Who might this AI impact?\n",
        "\n",
        "An imporant part of incorporating AI into your businesses is discuss how it would impact all areas of business.\n",
        "Let's come up with 3 groups people that would be impacted by an AI that can classify reviews as positive or negative. We will call these groups `stakeholders`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qISnUEcmumt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "stakeholder = '' #@param {type:\"string\"}\n",
        "stakeholder = '' #@param {type:\"string\"}\n",
        "stakeholder = '' #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLzQjhkpgoiY"
      },
      "source": [
        "\n",
        "\n",
        "*   **Discuss**: For each of those stakeholders, what are some benefits of this AI model? What are some drawbacks?\n",
        "\n",
        "\n",
        "> *Hint: What do each of those stakeholders care about?*\n",
        "\n",
        "\n",
        "\n",
        "* **Discuss**: What are some societal outcomes that can occur to if we had a lot of **false positives** (negative reviews misclassified as positive reviews)? How about **false negatives** (positive reviews misclassified as negative reviews)?\n",
        "\n",
        "*   **Discuss**: What are some potential sources of bias?\n",
        "\n",
        "*   **Discuss**: What are some other ethical questions you can come up with?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcqbg3NSNriB"
      },
      "source": [
        "# Optional Advanced Challenge: Linear Algebra and Embeddings\n",
        "\n",
        "(Heads-up: this challenge section is math-heavy!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTyeFBh0CbNO"
      },
      "source": [
        "One reason text embeddings are cool is that we can use them to explore connections in meaning between different words, including calculating similarity between words and completing [analogies](http://epsilon-it.utu.fi/wv_demo/).\n",
        "\n",
        "To get started, we'll first create a vocabulary of the most common words from our Yelp reviews dataset. We'll use a technique called the Bag of Words (BOW) model with a Counter Vectorizer, which counts how often each word appears. From this, we'll select the top 500 most frequently used words to form our vocabulary.\n",
        "\n",
        "Next, we'll create a dictionary containing the vectors for all the words in our vocabulary. This dictionary will help us analyze the relationships between words. If you want to use more than 500 words, feel free to change that number!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1obyb1GHppIV",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run this to define our vocabulary builder!\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab_dict(texts, top_n=500):\n",
        "    \"\"\"\n",
        "    Builds a dictionary of the most common words and their embeddings using SpaCy.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): The list of texts from which to build the vocabulary.\n",
        "        top_n (int): The number of top words to include in the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping words to their embeddings.\n",
        "    \"\"\"\n",
        "    # Tokenize the text and lower case each word\n",
        "    tokens = [word.lower() for text in texts for word in word_tokenize(text)]\n",
        "\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stopwords.words('english')]\n",
        "\n",
        "    # Count the occurrences of each word\n",
        "    word_counts = Counter(filtered_tokens)\n",
        "\n",
        "    # Select the top 'top_n' most common words\n",
        "    most_common_words = [word for word, count in word_counts.most_common(top_n)]\n",
        "\n",
        "    # Create a dictionary for the most common words and their embeddings\n",
        "    vocab_dict = {}\n",
        "    for word in most_common_words:\n",
        "        token = text_to_nlp.vocab[word]\n",
        "        if token.has_vector:  # Check if the token has a vector in the model's vocabulary\n",
        "            vocab_dict[word] = token.vector\n",
        "        else:\n",
        "            # Handle out-of-vocabulary words by assigning a zero vector\n",
        "            embedding_dim = text_to_nlp.vocab.vectors_length\n",
        "            vocab_dict[word] = np.zeros((embedding_dim,))\n",
        "\n",
        "    return vocab_dict\n",
        "\n",
        "# Example usage:\n",
        "# X_text_example = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
        "# vocab_dict = build_vocab_dict(X_text_example)\n",
        "# print(vocab_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHlGBkBKoUPI"
      },
      "outputs": [],
      "source": [
        "vocab_dict = build_vocab_dict(X_text, top_n = 800)\n",
        "\n",
        "for word, vec in vocab_dict.items():\n",
        "  print(word)\n",
        "\n",
        "print ('{} words in our dictionary'.format(len(vocab_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPCcIyTpDuab"
      },
      "source": [
        "### Cosine Similarity\n",
        "Next, let's calculate the similarity between two words, using their Word2Vec representations. As before, we'll use cosine similarity to measure the similarity between our vectors.\n",
        "\n",
        "As an example, imagine we had two three-dimensional vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omAmAv88GZUp"
      },
      "outputs": [],
      "source": [
        "v0 = [2,3,1]\n",
        "v1 = [2,4,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owbBQZUgGgjs"
      },
      "source": [
        "Run the code below to plot those vectors, and try changing the numbers above.\n",
        "How can you make a very small angle between the vectors? How can you make a very large angle?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QtbbBLgcFmE0"
      },
      "outputs": [],
      "source": [
        "#@title Run this to create an interactive 3D plot\n",
        "#NOTE: Would be extra cool with sliders for the vector coordinates! - DREW\n",
        "#Code from https://stackoverflow.com/questions/47319238/python-plot-3d-vectors\n",
        "import numpy as np\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "def vector_plot(tvects,is_vect=True,orig=[0,0,0]):\n",
        "    \"\"\"Plot vectors using plotly\"\"\"\n",
        "\n",
        "    if is_vect:\n",
        "        if not hasattr(orig[0],\"__iter__\"):\n",
        "            coords = [[orig,np.sum([orig,v],axis=0)] for v in tvects]\n",
        "        else:\n",
        "            coords = [[o,np.sum([o,v],axis=0)] for o,v in zip(orig,tvects)]\n",
        "    else:\n",
        "        coords = tvects\n",
        "\n",
        "    data = []\n",
        "    for i,c in enumerate(coords):\n",
        "        X1, Y1, Z1 = zip(c[0])\n",
        "        X2, Y2, Z2 = zip(c[1])\n",
        "        vector = go.Scatter3d(x = [X1[0],X2[0]],\n",
        "                              y = [Y1[0],Y2[0]],\n",
        "                              z = [Z1[0],Z2[0]],\n",
        "                              marker = dict(size = [0,5],\n",
        "                                            color = ['blue'],\n",
        "                                            line=dict(width=5,\n",
        "                                                      color='DarkSlateGrey')),\n",
        "                              name = 'Vector'+str(i+1))\n",
        "        data.append(vector)\n",
        "\n",
        "    layout = go.Layout(\n",
        "             margin = dict(l = 4,\n",
        "                           r = 4,\n",
        "                           b = 4,\n",
        "                           t = 4)\n",
        "                  )\n",
        "    fig = go.Figure(data=data,layout=layout)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "vector_plot([v0,v1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ2JQmZELItA"
      },
      "source": [
        "## Exercise: Identifying Similar Words Using Your Cosine Similarity Function\n",
        "\n",
        "In this exercise, you will apply your own implementation of cosine similarity to find the most similar word to a given target word in a vocabulary. You‚Äôll be using the `my_cosine_similarity` function that you wrote earlier, leveraging it to compare word vectors and identify the closest matches.\n",
        "\n",
        "### What You'll Do\n",
        "\n",
        "Write a function named `find_most_similar` that utilizes your `my_cosine_similarity` function to determine which word in a predefined vocabulary is most similar to a specified target word. The function should return both the most similar word and its similarity score!\n",
        "\n",
        "### Some Guidelines\n",
        "\n",
        "1. **Check Vocabulary**: Initially, ensure the target word is present in the vocabulary. If it‚Äôs not, the function should notify the user and not proceed with calculations.\n",
        "2. **Calculate Similarity**: Use your `my_cosine_similarity` function to compute the similarity between the target word's vector and each vector in the vocabulary.\n",
        "3. **Track the Highest Score**: As you compute similarities, keep track of the word with the highest similarity score.\n",
        "4. **Return Results**: After checking all words, return the word with the highest similarity score and the score itself.\n",
        "\n",
        "Here's an example of how your code will be used!\n",
        "\n",
        "```python\n",
        "similar_word, similarity_score = find_most_similar('burger')\n",
        "if similar_word is not None:\n",
        "    print(f\"The most similar word to 'burger' is '{similar_word}' with a similarity score of {similarity_score:.2f}.\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwC6EioNJHKR"
      },
      "outputs": [],
      "source": [
        "def find_most_similar(target_word):\n",
        "    # Check if the target word is in the vocabulary dictionary\n",
        "    if target_word not in vocab_dict:\n",
        "        print(\"Word not in dictionary\")\n",
        "        return None, None\n",
        "\n",
        "    # Retrieve the vector for the target word from the vocabulary dictionary\n",
        "    vec1 = vocab_dict[target_word]\n",
        "\n",
        "    # Initialize variables to keep track of the most similar word and the highest similarity score\n",
        "    most_similar_word = None\n",
        "    highest_similarity = -np.inf  # Start with the lowest possible similarity\n",
        "\n",
        "    # Iterate over each word and its vector in the vocabulary dictionary\n",
        "    for word, vec2 in vocab_dict.items():\n",
        "        # YOUR CODE HERE: Calculate the similarity using the my_cosine_similarity function\n",
        "        # Make sure to remove the continue\n",
        "        continue\n",
        "    # Return the most similar word along with the similarity score\n",
        "    return most_similar_word, highest_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5boGdBCC0qU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "def find_most_similar(target_word):\n",
        "    # Check if the target word is in the vocabulary dictionary\n",
        "    if target_word not in vocab_dict:\n",
        "        print(\"Word not in dictionary\")\n",
        "        return None, None\n",
        "\n",
        "    # Retrieve the vector for the target word from the vocabulary dictionary\n",
        "    vec1 = vocab_dict[target_word]\n",
        "\n",
        "    # Initialize variables to keep track of the most similar word and the highest similarity score\n",
        "    most_similar_word = None\n",
        "    highest_similarity = -np.inf  # Start with the lowest possible similarity\n",
        "\n",
        "    # Iterate over each word and its vector in the vocabulary dictionary\n",
        "    for word, vec2 in vocab_dict.items():\n",
        "        if word == target_word:\n",
        "            continue  # Skip comparison with the target word itself\n",
        "\n",
        "        # Calculate the similarity using the my_cosine_similarity function\n",
        "        similarity = my_cosine_similarity(vec1, vec2)\n",
        "\n",
        "        # Update the most similar word and the highest similarity score\n",
        "        if similarity > highest_similarity:\n",
        "            highest_similarity = similarity\n",
        "            most_similar_word = word\n",
        "\n",
        "    # Return the most similar word along with the similarity score\n",
        "    return most_similar_word, highest_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSNAwNv3Ru8D"
      },
      "source": [
        "### Let's test your function below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJmAeGIPJA3z"
      },
      "outputs": [],
      "source": [
        "word = \"eat\" #@param {type:'string'}\n",
        "\n",
        "similar_word, similarity_score = find_most_similar(word)\n",
        "if similar_word is not None:\n",
        "    print(f\"The most similar word to '{word}' is '{similar_word}' with a similarity score of {similarity_score:.2f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxcyY1YZO9u5"
      },
      "source": [
        "## Using Word Analogies\n",
        "\n",
        "We can use the functions we've built to complete word analogies, similar to the examples found [here](http://epsilon-it.utu.fi/wv_demo/). For instance, consider the analogy:\n",
        "\n",
        "- Breakfast is to bagel as lunch is to ________,\n",
        "\n",
        "This involves a bit of \"word arithmetic\". Suppose $A_1$, $A_2$, and $B_1$ are vectors representing three known words. Our task is to find $B_2$ to complete the analogy:\n",
        "\n",
        "- $A_1$ is to $A_2$ as $B_1$ is to $B_2$.\n",
        "\n",
        "Intuitively, this implies that the vector difference between $A_1$ and $A_2$ should be the same as the vector difference between $B_1$ and $B_2$. Thus, we can express this relationship mathematically as:\n",
        "\n",
        "- $A_1 - A_2 = B_1 - B_2$\n",
        "\n",
        "### Solving for $B_2$:\n",
        "\n",
        "To find $B_2$, we rearrange the above equation:\n",
        "\n",
        "- $B_2 = B_1 - (A_1 - A_2)$\n",
        "\n",
        "This formulation allows us to compute the expected vector for $B_2$ directly by using vector arithmetic. Once we have the vector for $B_2$, we can use our previously developed functions to identify the word whose vector representation is closest to this computed vector. Try it out and explore different analogies!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSMQEm1JQkVZ"
      },
      "outputs": [],
      "source": [
        "# Complete the function below!\n",
        "def find_analogy(word_a1, word_a2, word_b1):\n",
        "    # Retrieve vectors for each word\n",
        "    # Use the word2vec function to get the vector for each word\n",
        "    a1 = word2vec(word_a1)\n",
        "    a2 = word2vec(word_a2)\n",
        "    b1 = word2vec(word_b1)\n",
        "\n",
        "    # Check if any vectors are None (word not in vocabulary)\n",
        "    # If any of the words are not in the vocabulary, print a message and return None\n",
        "    if a1 is None or a2 is None or b1 is None:\n",
        "        missing = [word for word, vec in zip([word_a1, word_a2, word_b1], [a1, a2, b1]) if vec is None]\n",
        "        print(f\"Missing vector for: {', '.join(missing)}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate the expected vector for b2 based on the analogy\n",
        "    # The analogy is: word_a1 is to word_a2 as word_b1 is to what word?\n",
        "    # Calculate vec1 by subtracting the difference between a1 and a2 from b1\n",
        "    vec1 = b1 - (a1 - a2)\n",
        "\n",
        "    # Initialize variables to keep track of the most similar word and the highest similarity score\n",
        "    most_similar_word = None\n",
        "    highest_similarity = None  # Initialize with None or a very low value\n",
        "\n",
        "    # Iterate over each word and its vector in the vocabulary dictionary\n",
        "    # vocab_dict is a dictionary where keys are words and values are their vectors\n",
        "    for word, vec2 in vocab_dict.items():\n",
        "        # Skip the current word_b1 to avoid trivial matches\n",
        "\n",
        "        # Calculate the similarity using the my_cosine_similarity function\n",
        "        # Your code to calculate similarity goes here\n",
        "\n",
        "        # Update the most similar word and the highest similarity score if the current word is more similar\n",
        "        # Your code to update most_similar_word and highest_similarity goes here\n",
        "\n",
        "    # Return the most similar word along with the similarity score\n",
        "    return most_similar_word, highest_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfTeKGqYLw_1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Instructor Solution\n",
        "\n",
        "def find_analogy(word_a1, word_a2, word_b1):\n",
        "\n",
        "    # Retrieve vectors for each word\n",
        "    a1 = word2vec(word_a1)\n",
        "    a2 = word2vec(word_a2)\n",
        "    b1 = word2vec(word_b1)\n",
        "\n",
        "    # Check if any vectors are None (word not in vocabulary)\n",
        "\n",
        "    if a1 is None or a2 is None or b1 is None:\n",
        "        missing = [word for word, vec in zip([word_a1, word_a2, word_b1], [a1, a2, b1]) if vec is None]\n",
        "        print(f\"Missing vector for: {', '.join(missing)}\")\n",
        "        return None\n",
        "\n",
        "    # Calculate the expected vector for b2 based on the analogy\n",
        "    vec1 = b1 - (a1 - a2)\n",
        "\n",
        "        # Initialize variables to keep track of the most similar word and the highest similarity score\n",
        "    most_similar_word = None\n",
        "    highest_similarity = -np.inf  # Start with the lowest possible similarity\n",
        "\n",
        "    # Iterate over each word and its vector in the vocabulary dictionary\n",
        "    for word, vec2 in vocab_dict.items():\n",
        "        if word == word_b1: continue\n",
        "        # Calculate the similarity using the my_cosine_similarity function\n",
        "        similarity = my_cosine_similarity(vec1, vec2)\n",
        "\n",
        "        # Update the most similar word and the highest similarity score\n",
        "        if similarity > highest_similarity:\n",
        "            highest_similarity = similarity\n",
        "            most_similar_word = word\n",
        "\n",
        "    # Return the most similar word along with the similarity score\n",
        "    return most_similar_word, highest_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHmE4xoyQHYN"
      },
      "source": [
        "### Let's test your function to see how it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbazWL9BPDYQ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "worda1 = \"cars\" #@param {type:'string'}\n",
        "worda2 = \"wheels\" #@param {type:'string'}\n",
        "wordb1 = \"birds\" #@param {type:'string'}\n",
        "\n",
        "similar_word, similarity_score = find_analogy(worda1, worda2, wordb1)\n",
        "if similar_word is not None:\n",
        "    print(f\"The word analogous to '{wordb1}' in the context of '{worda1}' to '{worda2}' is '{similar_word}', with a similarity score of {similarity_score:.2f}.\")\n",
        "else:\n",
        "    print(f\"No analogous word found for '{wordb1}' in the context of '{worda1}' to '{worda2}'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlDcBn9kQxNR"
      },
      "source": [
        "Word arithmetic doesn't always work perfectly - it's pretty tricky to find good examples! Which can you discover?\n",
        "\n",
        "If you're looking for a way to expand further on this exercise, you can try seeing what happens when you use [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), another common measurement, instead of cosine similarity."
      ]
    }
  ]
}